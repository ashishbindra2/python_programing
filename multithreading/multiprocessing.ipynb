{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db35709d",
   "metadata": {},
   "source": [
    "Thing should be done:\n",
    " - first should without any module\n",
    "      - with async io\n",
    "      - with threading\n",
    "      - with process\n",
    "- make some methods which are:\n",
    "    1. just read and write file frome one folder to another   \n",
    "    2. insert data one by one atleast 1000 row\n",
    "    3. run flask app with 2 web scrapper\n",
    "    4. perform have caluclation like fibnachi \n",
    "     \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3954655f",
   "metadata": {},
   "source": [
    "Parller programing\n",
    "----------------\n",
    "Q1. How to unlocak and use all of your CPU cores\n",
    "\n",
    "There are Three big contenders for dow to deal with multiple task in Python\n",
    "1. asyncio\n",
    "        - primarily concerned with I/O - bound operations\n",
    "        - asyncio built to allow task to cooperatively pause themseleves allow other task to run particularly while they doing nothing, just waiting.\n",
    "        - cooporative pasusing/waiting\n",
    "        - Good for IO bound\n",
    "2. threading\n",
    "3. multiprocessing\n",
    "\n",
    "ETL = > Extract Transform Load\n",
    "example\n",
    "- use are using SciPy to read audio file as a NumPy array.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588e7609",
   "metadata": {},
   "source": [
    "#### Throw away varable ( _ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8573b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faker\n",
      "  Obtaining dependency information for faker from https://files.pythonhosted.org/packages/65/23/8f2f005cf4ef6df15466f6d906294c6c4bac1add6e7c750cd755da5e4419/Faker-25.2.0-py3-none-any.whl.metadata\n",
      "  Downloading Faker-25.2.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.4 in c:\\users\\bindra\\anaconda3\\lib\\site-packages (from faker) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\bindra\\anaconda3\\lib\\site-packages (from python-dateutil>=2.4->faker) (1.16.0)\n",
      "Downloading Faker-25.2.0-py3-none-any.whl (1.8 MB)\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.0/1.8 MB 1.4 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.2/1.8 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.6/1.8 MB 5.6 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.2/1.8 MB 7.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.7/1.8 MB 8.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.8/1.8 MB 8.2 MB/s eta 0:00:00\n",
      "Installing collected packages: faker\n",
      "Successfully installed faker-25.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ca535f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "import json\n",
    "\n",
    "fk = Faker('en_IN')\n",
    "for i in range(1,10000):\n",
    "    data = fk.simple_profile()\n",
    "    json_string = json.dumps(data, indent=4, default=str)\n",
    "\n",
    "    with open(f'test/f1/file_{i}.json','w') as outfile:\n",
    "        outfile.write(json_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f924ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken 5.14 Sec\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "\n",
    "def do_something():\n",
    "    for i in range(10000):\n",
    "        data = None\n",
    "        \n",
    "        with open(f'test/f1/file_{i}.json','r') as outfile:\n",
    "            data = json.load(outfile)\n",
    "        with open(f'test/f2/file_{i}.json','w') as outfile:\n",
    "            json.dump(data,outfile)    \n",
    "\n",
    "start = time.perf_counter()\n",
    "do_something()\n",
    "end = time.perf_counter()\n",
    "time = end- start\n",
    "\n",
    "print(f\"time taken {time:.2f} Sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f012ba42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken 4.52 Sec\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "\n",
    "def do_something(i):\n",
    "    data = None\n",
    "    with open(f'test/f1/file_{i}.json','r') as outfile:\n",
    "        data = json.load(outfile)\n",
    "    with open(f'test/f2/file_{i}.json','w') as outfile:\n",
    "        json.dump(data,outfile)    \n",
    "\n",
    "start = time.perf_counter()\n",
    "\n",
    "for i in range(10000):\n",
    "    do_something(i)\n",
    "\n",
    "end = time.perf_counter()\n",
    "time = end- start\n",
    "\n",
    "print(f\"time taken {time:.2f} Sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ec519df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken 4.35 Sec\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "\n",
    "async def do_something(i):\n",
    "    data = None\n",
    "    with open(f'test/f1/file_{i}.json','r') as outfile:\n",
    "        data = json.load(outfile)\n",
    "    with open(f'test/f2/file_{i}.json','w') as outfile:\n",
    "        json.dump(data,outfile)    \n",
    "\n",
    "start = time.perf_counter()\n",
    "\n",
    "for i in range(10000):\n",
    "    await do_something(i)\n",
    "\n",
    "end = time.perf_counter()\n",
    "time = end - start\n",
    "\n",
    "print(f\"time taken {time:.2f} Sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c465257a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken 4.48 Sec\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import threading\n",
    "\n",
    "def do_something(i):\n",
    "    data = None\n",
    "    with open(f'test/f1/file_{i}.json','r') as outfile:\n",
    "        data = json.load(outfile)\n",
    "    with open(f'test/f2/file_{i}.json','w') as outfile:\n",
    "        json.dump(data,outfile)    \n",
    "\n",
    "start = time.perf_counter()\n",
    "\n",
    "for i in range(10000):\n",
    "    threading.Thread( target =do_something,args=(i,)).start()\n",
    "\n",
    "end = time.perf_counter()\n",
    "time = end - start\n",
    "\n",
    "print(f\"time taken {time:.2f} Sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff8ff0f",
   "metadata": {},
   "source": [
    "- In order to process to run we need to use start method on each one \n",
    "- it might not do what we think it will do it run function\n",
    "- while 2 process where sleeping our scrpit were continue runing and calucluate out and printed out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0e7c80b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!!!\n",
      "Done!!!\n",
      "Finished in 0.00 Secound(s)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "# import threading\n",
    "import multiprocessing\n",
    "\n",
    "def do_something(i):\n",
    "    data = None\n",
    "    with open(f'test/f1/file_{i}.json','r') as outfile:\n",
    "        data = json.load(outfile)\n",
    "    with open(f'test/f2/file_{i}.json','w') as outfile:\n",
    "        json.dump(data,outfile)    \n",
    "    print(\"Done!!!\")\n",
    "    \n",
    "start = time.perf_counter()\n",
    "do_something(1)\n",
    "do_something(2)\n",
    "\n",
    "end = time.perf_counter()\n",
    "time = end - start\n",
    "\n",
    "print(f\"Finished in {time:.2f} Secound(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9990a85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 0.09 Secound(s)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "# import threading\n",
    "import multiprocessing\n",
    "\n",
    "def do_something(i):\n",
    "    data = None\n",
    "    with open(f'test/f1/file_{i}.json','r') as outfile:\n",
    "        data = json.load(outfile)\n",
    "    with open(f'test/f2/file_{i}.json','w') as outfile:\n",
    "        json.dump(data,outfile)    \n",
    "    print(\"Done!!!\")\n",
    "    \n",
    "start = time.perf_counter()\n",
    "# for i in range(10000):\n",
    "p1 = multiprocessing.Process(target =do_something,args=(1,))\n",
    "p2 = multiprocessing.Process(target =do_something,args=(2,))\n",
    "\n",
    "p1.start()\n",
    "p2.start()\n",
    "\n",
    "p1.join()\n",
    "p2.join()\n",
    "\n",
    "end = time.perf_counter()\n",
    "time = end - start\n",
    "\n",
    "print(f\"Finished in {time:.2f} Secound(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42d8ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "The arguments must be serialise using pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4cf4eb32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 0.21 Secound(s)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "# import threading\n",
    "import multiprocessing\n",
    "\n",
    "def do_something(i):\n",
    "    data = None\n",
    "    with open(f'test/f1/file_{i}.json','r') as outfile:\n",
    "        data = json.load(outfile)\n",
    "    with open(f'test/f2/file_{i}.json','w') as outfile:\n",
    "        json.dump(data,outfile)    \n",
    "    print(\"Done!!!\")\n",
    "    \n",
    "start = time.perf_counter()\n",
    "\n",
    "processes = []\n",
    "for i in range(10):\n",
    "    p = multiprocessing.Process(target =do_something,args=(i,))\n",
    "    p.start()\n",
    "    processes.append(p)\n",
    "\n",
    "for process in processes:\n",
    "    process.join()\n",
    "\n",
    "end = time.perf_counter()\n",
    "time = end - start\n",
    "\n",
    "print(f\"Finished in {time:.2f} Secound(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2f47a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "If we want to execute function once at a time then we can use the submit method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "877fe7a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "BrokenProcessPool",
     "evalue": "A process in the process pool was terminated abruptly while the future was running or pending.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBrokenProcessPool\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mProcessPoolExecutor() \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[0;32m     19\u001b[0m     f1 \u001b[38;5;241m=\u001b[39m executor\u001b[38;5;241m.\u001b[39msubmit(do_something,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(f1\u001b[38;5;241m.\u001b[39mresult())\n\u001b[0;32m     21\u001b[0m     f2 \u001b[38;5;241m=\u001b[39m executor\u001b[38;5;241m.\u001b[39msubmit(do_something,\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(f2\u001b[38;5;241m.\u001b[39mresult())\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\concurrent\\futures\\_base.py:456\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mBrokenProcessPool\u001b[0m: A process in the process pool was terminated abruptly while the future was running or pending."
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "import time\n",
    "import json\n",
    "# import threading\n",
    "import multiprocessing\n",
    "\n",
    "def do_something(i):\n",
    "    data = None\n",
    "    print(i)\n",
    "    with open(f'test/f1/file_{i}.json','r') as outfile:\n",
    "        data = json.load(outfile)\n",
    "    with open(f'test/f2/file_{i}.json','w') as outfile:\n",
    "        json.dump(data,outfile)    \n",
    "    return \"Done.....!!!\"\n",
    "    \n",
    "start = time.perf_counter()\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    f1 = executor.submit(do_something,1)\n",
    "    print(f1.result())\n",
    "    f2 = executor.submit(do_something,1)\n",
    "    print(f2.result())\n",
    "\n",
    "\n",
    "end = time.perf_counter()\n",
    "time = end - start\n",
    "\n",
    "print(f\"Finished in {time:.2f} Secound(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "06922ec3",
   "metadata": {},
   "outputs": [
    {
     "ename": "BrokenProcessPool",
     "evalue": "A process in the process pool was terminated abruptly while the future was running or pending.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBrokenProcessPool\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m     results \u001b[38;5;241m=\u001b[39m [executor\u001b[38;5;241m.\u001b[39msubmit(do_something,\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m)]\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mas_completed(results):\n\u001b[1;32m---> 22\u001b[0m         \u001b[38;5;28mprint\u001b[39m(f\u001b[38;5;241m.\u001b[39mresult())\n\u001b[0;32m     25\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[0;32m     26\u001b[0m time \u001b[38;5;241m=\u001b[39m end \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\concurrent\\futures\\_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mBrokenProcessPool\u001b[0m: A process in the process pool was terminated abruptly while the future was running or pending."
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "import time\n",
    "import json\n",
    "# import threading\n",
    "# import multiprocessing\n",
    "\n",
    "def do_something(i):\n",
    "    data = None\n",
    "    print(i)\n",
    "    with open(f'test/f1/file_{i}.json','r') as outfile:\n",
    "        data = json.load(outfile)\n",
    "    with open(f'test/f2/file_{i}.json','w') as outfile:\n",
    "        json.dump(data,outfile)    \n",
    "    return f\"Done.....{i}!!!\"\n",
    "    \n",
    "start = time.perf_counter()\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    results = [executor.submit(do_something,1) for _ in range(10)]\n",
    "    \n",
    "    for f in concurrent.futures.as_completed(results):\n",
    "        print(f.result())\n",
    "  \n",
    "\n",
    "end = time.perf_counter()\n",
    "time = end - start\n",
    "\n",
    "print(f\"Finished in {time:.2f} Secound(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a3ea24",
   "metadata": {},
   "outputs": [],
   "source": [
    "as_complete nethod execute inorder the process is completed, \n",
    "submited each function at a time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8e41673d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "Generated an exception: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "Generated an exception: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "Generated an exception: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "Generated an exception: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "Finished in 0.16 Secound(s)\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "import time\n",
    "import json\n",
    "# import threading\n",
    "import multiprocessing\n",
    "\n",
    "def do_something(i):\n",
    "    data = None\n",
    "    print(i)\n",
    "    with open(f'test/f1/file_{i}.json','r') as outfile:\n",
    "        data = json.load(outfile)\n",
    "    with open(f'test/f2/file_{i}.json','w') as outfile:\n",
    "        json.dump(data,outfile)    \n",
    "    return f\"Done.....{i}!!!\"\n",
    "    \n",
    "start = time.perf_counter()\n",
    "print(multiprocessing.cpu_count())\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers = min(32, multiprocessing.cpu_count()-3)) as executor:\n",
    "    results = [executor.submit(do_something,i) for i in range(1,5)]\n",
    "    \n",
    "    for future in concurrent.futures.as_completed(results):\n",
    "        try:\n",
    "            result = future.result()\n",
    "            print(result)\n",
    "        except Exception as exc:\n",
    "            print(f\"Generated an exception: {exc}\")\n",
    "  \n",
    "\n",
    "end = time.perf_counter()\n",
    "time = end - start\n",
    "\n",
    "print(f\"Finished in {time:.2f} Secound(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1fae14",
   "metadata": {},
   "source": [
    "We can  of values to run our function over list use built in map method\n",
    "- it return the results and the order thatthey we\n",
    "- it won't actually raise that exection while the running the process \n",
    "- the exception will be raised when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ad8f4f71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "ename": "BrokenProcessPool",
     "evalue": "A process in the process pool was terminated abruptly while the future was running or pending.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBrokenProcessPool\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mProcessPoolExecutor() \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[0;32m     19\u001b[0m     results \u001b[38;5;241m=\u001b[39m executor\u001b[38;5;241m.\u001b[39mmap(do_something, \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m))\n\u001b[1;32m---> 21\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;28mprint\u001b[39m(result)\n\u001b[0;32m     24\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\concurrent\\futures\\process.py:606\u001b[0m, in \u001b[0;36m_chain_from_iterable_of_lists\u001b[1;34m(iterable)\u001b[0m\n\u001b[0;32m    600\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_chain_from_iterable_of_lists\u001b[39m(iterable):\n\u001b[0;32m    601\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    602\u001b[0m \u001b[38;5;124;03m    Specialized implementation of itertools.chain.from_iterable.\u001b[39;00m\n\u001b[0;32m    603\u001b[0m \u001b[38;5;124;03m    Each item in *iterable* should be a list.  This function is\u001b[39;00m\n\u001b[0;32m    604\u001b[0m \u001b[38;5;124;03m    careful not to keep references to yielded objects.\u001b[39;00m\n\u001b[0;32m    605\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 606\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m element \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m    607\u001b[0m         element\u001b[38;5;241m.\u001b[39mreverse()\n\u001b[0;32m    608\u001b[0m         \u001b[38;5;28;01mwhile\u001b[39;00m element:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\concurrent\\futures\\_base.py:619\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[1;34m()\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[0;32m    618\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 619\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs\u001b[38;5;241m.\u001b[39mpop())\n\u001b[0;32m    620\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    621\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs\u001b[38;5;241m.\u001b[39mpop(), end_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic())\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\concurrent\\futures\\_base.py:317\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 317\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m fut\u001b[38;5;241m.\u001b[39mresult(timeout)\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    319\u001b[0m         fut\u001b[38;5;241m.\u001b[39mcancel()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\concurrent\\futures\\_base.py:456\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mBrokenProcessPool\u001b[0m: A process in the process pool was terminated abruptly while the future was running or pending."
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "import time\n",
    "import json\n",
    "# import threading\n",
    "import multiprocessing\n",
    "\n",
    "def do_something(i):\n",
    "    data = None\n",
    "    print(i)\n",
    "#     with open(f'test/f1/file_{i}.json','r') as outfile:\n",
    "#         data = json.load(outfile)\n",
    "#     with open(f'test/f2/file_{i}.json','w') as outfile:\n",
    "#         json.dump(data,outfile)    \n",
    "    return f\"Done.....{i}!!!\"\n",
    "    \n",
    "start = time.perf_counter()\n",
    "print(multiprocessing.cpu_count())\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    results = executor.map(do_something, range(10))\n",
    "    \n",
    "    for result in results:\n",
    "        print(result)\n",
    "\n",
    "end = time.perf_counter()\n",
    "time = end - start\n",
    "\n",
    "print(f\"Finished in {time:.2f} Secound(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fcad3a27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 0.22236659994814545 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "import concurrent.futures\n",
    "from PIL import Image, ImageFilter\n",
    "\n",
    "img_names = [\n",
    "    'photo-1516117172878-fd2c41f4a759.jpg',\n",
    "    'photo-1532009324734-20a7a5813719.jpg',\n",
    "    'photo-1524429656589-6633a470097c.jpg',\n",
    "    'photo-1530224264768-7ff8c1789d79.jpg',\n",
    "    'photo-1564135624576-c5c88640f235.jpg',\n",
    "    'photo-1541698444083-023c97d3f4b6.jpg',\n",
    "    'photo-1522364723953-452d3431c267.jpg',\n",
    "    'photo-1513938709626-033611b8cc03.jpg',\n",
    "    'photo-1507143550189-fed454f93097.jpg',\n",
    "    'photo-1493976040374-85c8e12f0c0e.jpg',\n",
    "    'photo-1504198453319-5ce911bafcde.jpg',\n",
    "    'photo-1530122037265-a5f1f91d3b99.jpg',\n",
    "    'photo-1516972810927-80185027ca84.jpg',\n",
    "    'photo-1550439062-609e1531270e.jpg',\n",
    "    'photo-1549692520-acc6669e2f0c.jpg'\n",
    "]\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "\n",
    "size = (1200, 1200)\n",
    "\n",
    "\n",
    "def process_image(img_name):\n",
    "    print(f'test/img/{img_name}')\n",
    "    img = Image.open(f'test/img/{img_name}')\n",
    "\n",
    "    img = img.filter(ImageFilter.GaussianBlur(15))\n",
    "\n",
    "    img.thumbnail(size)\n",
    "    img.save(f'test/img/processed/{img_name}')\n",
    "    print(f'{img_name} was processed...')\n",
    "\n",
    "\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    executor.map(process_image, img_names)\n",
    "\n",
    "\n",
    "t2 = time.perf_counter()\n",
    "\n",
    "print(f'Finished in {t2-t1} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a0c12fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-1516117172878-fd2c41f4a759.jpg was processed...\n",
      "photo-1516972810927-80185027ca84.jpg was processed...\n",
      "photo-1524429656589-6633a470097c.jpg was processed...\n",
      "photo-1522364723953-452d3431c267.jpg was processed...\n",
      "photo-1530122037265-a5f1f91d3b99.jpg was processed...\n",
      "photo-1532009324734-20a7a5813719.jpg was processed...\n",
      "photo-1530224264768-7ff8c1789d79.jpg was processed...\n",
      "photo-1541698444083-023c97d3f4b6.jpg was processed...\n",
      "photo-1564135624576-c5c88640f235.jpg was processed...\n",
      "photo-1550439062-609e1531270e.jpg was processed...\n",
      "photo-1549692520-acc6669e2f0c.jpg was processed...\n",
      "photo-1504198453319-5ce911bafcde.jpg was processed...\n",
      "photo-1493976040374-85c8e12f0c0e.jpg was processed...\n",
      "Finished in 6.170788799994625 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "import concurrent.futures\n",
    "from PIL import Image, ImageFilter\n",
    "\n",
    "img_names = [\n",
    "    'photo-1516117172878-fd2c41f4a759.jpg',\n",
    "    'photo-1532009324734-20a7a5813719.jpg',\n",
    "    'photo-1524429656589-6633a470097c.jpg',\n",
    "    'photo-1530224264768-7ff8c1789d79.jpg',\n",
    "    'photo-1564135624576-c5c88640f235.jpg',\n",
    "    'photo-1541698444083-023c97d3f4b6.jpg',\n",
    "    'photo-1522364723953-452d3431c267.jpg',\n",
    "    'photo-1513938709626-033611b8cc03.jpg',\n",
    "    'photo-1507143550189-fed454f93097.jpg',\n",
    "    'photo-1493976040374-85c8e12f0c0e.jpg',\n",
    "    'photo-1504198453319-5ce911bafcde.jpg',\n",
    "    'photo-1530122037265-a5f1f91d3b99.jpg',\n",
    "    'photo-1516972810927-80185027ca84.jpg',\n",
    "    'photo-1550439062-609e1531270e.jpg',\n",
    "    'photo-1549692520-acc6669e2f0c.jpg'\n",
    "]\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "\n",
    "size = (1200, 1200)\n",
    "\n",
    "\n",
    "def process_image(img_name):\n",
    "    img = Image.open(f'test/img/{img_name}')\n",
    "\n",
    "    img = img.filter(ImageFilter.GaussianBlur(15))\n",
    "\n",
    "    img.thumbnail(size)\n",
    "    img.save(f'test/img/processed/{img_name}')\n",
    "    print(f'{img_name} was processed...')\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    executor.map(process_image, img_names)\n",
    "\n",
    "t2 = time.perf_counter()\n",
    "\n",
    "print(f'Finished in {t2-t1} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff78f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "from multiprocessing import Pool\n",
    "from PIL import Image, ImageFilter\n",
    "\n",
    "img_names = [\n",
    "    'photo-1516117172878-fd2c41f4a759.jpg',\n",
    "    'photo-1532009324734-20a7a5813719.jpg',\n",
    "    'photo-1524429656589-6633a470097c.jpg',\n",
    "    'photo-1530224264768-7ff8c1789d79.jpg',\n",
    "    'photo-1564135624576-c5c88640f235.jpg',\n",
    "    'photo-1541698444083-023c97d3f4b6.jpg',\n",
    "    'photo-1522364723953-452d3431c267.jpg',\n",
    "    'photo-1513938709626-033611b8cc03.jpg',\n",
    "    'photo-1507143550189-fed454f93097.jpg',\n",
    "    'photo-1493976040374-85c8e12f0c0e.jpg',\n",
    "    'photo-1504198453319-5ce911bafcde.jpg',\n",
    "    'photo-1530122037265-a5f1f91d3b99.jpg',\n",
    "    'photo-1516972810927-80185027ca84.jpg',\n",
    "    'photo-1550439062-609e1531270e.jpg',\n",
    "    'photo-1549692520-acc6669e2f0c.jpg'\n",
    "]\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "\n",
    "size = (1200, 1200)\n",
    "\n",
    "\n",
    "def process_image(img_name):\n",
    "    print(f'test/img/{img_name}')\n",
    "    img = Image.open(f'test/img/{img_name}')\n",
    "\n",
    "    img = img.filter(ImageFilter.GaussianBlur(15))\n",
    "\n",
    "    img.thumbnail(size)\n",
    "    img.save(f'test/img/processed/{img_name}')\n",
    "    print(f'{img_name} was processed...')\n",
    "\n",
    "\n",
    "\n",
    "with Pool() as executor:\n",
    "    executor.map(process_image, img_names)\n",
    "\n",
    "\n",
    "t2 = time.perf_counter()\n",
    "\n",
    "print(f'Finished in {t2-t1} seconds')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
